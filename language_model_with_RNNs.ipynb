{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Improve an Language Model with PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training a Language Model with RNNs and LSTMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068\n",
      "[['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>'], ['pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>'], ['mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>'], ['rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>'], ['a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', 'to', 'it', 'more', 'than', 'N', 'years', 'ago', 'researchers', 'reported', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "# Read the training, validation, and testing files, and replace newlines with the \"<eos>\" token.\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        lines = [line.split() + ['<eos>'] for line in lines]\n",
    "        return lines\n",
    "ptb_train_lines = read_file('penn-treebank/ptb.train.txt')\n",
    "ptb_valid_lines = read_file('penn-treebank/ptb.valid.txt')\n",
    "ptb_test_lines = read_file('penn-treebank/ptb.test.txt')\n",
    "print(len(ptb_train_lines))\n",
    "print(ptb_train_lines[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929589\n",
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all articles in the training set into one long sequence.\n",
    "ptb_train_tokens = [token for line in ptb_train_lines for token in line]\n",
    "print(len(ptb_train_tokens))\n",
    "print(ptb_train_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batches with shape (batch size, sequence_length + 1).\n",
    "# Continuous Batching\n",
    "# use PyTorch's DataLoader class but overwrite the \"iter\" method.\n",
    "# The \"iter\" method is called when we use a for loop to iterate over the DataLoader object.\n",
    "# We will overwrite this method to return a generator that yields batches of data.\n",
    "# The \"init_epoch\" method is called at the end of each epoch to shuffle the data.\n",
    "# The \"shuffle_batches\" method shuffles the batches.\n",
    "# The \"__len__\" method returns the number of batches in the dataset.\n",
    "# The \"__getitem__\" method returns a batch of data given an index.\n",
    "\n",
    "class PennTreebankDataset(Dataset):\n",
    "    def __init__(self, data_lines, sequence_length, batch_size):\n",
    "        self.data_lines = data_lines\n",
    "        self.data = None\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.split_length = None\n",
    "        self.init_epoch()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.split_length  // self.sequence_length * self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Split the long sequence into batch size equal-length sequences\n",
    "        # For each split sequence, cut them into continuous parts\n",
    "        # Batch k is composed of the kth part of all the sequences.\n",
    "        batch_x = idx % self.batch_size\n",
    "        batch_y = idx // self.batch_size\n",
    "        inputs = self.data[batch_x * self.split_length + batch_y*self.sequence_length: batch_x * self.split_length + (batch_y+1)*self.sequence_length]\n",
    "        outputs = self.data[batch_x * self.split_length + batch_y*self.sequence_length + 1: batch_x * self.split_length + (batch_y+1)*self.sequence_length + 1]\n",
    "        return inputs, outputs\n",
    "    \n",
    "    def init_epoch(self):\n",
    "        self.data = self.shuffle_lines(self.data_lines)\n",
    "        self.split_length = len(self.data) // self.batch_size\n",
    "\n",
    "    def shuffle_lines(self, data_lines):\n",
    "        np.random.shuffle(data_lines)\n",
    "        data = [token for line in data_lines for token in line]\n",
    "        return self.tensor_from_tokens(data)\n",
    "    \n",
    "    def tensor_from_tokens(self, tokens):\n",
    "        token_ids = [vocab[token] for token in tokens]\n",
    "        return torch.tensor(token_ids, dtype=torch.long, device=device)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove embeddings...\n",
      "Done.\n",
      "torch.Size([400000, 300])\n",
      "tensor([ 4.6560e-02,  2.1318e-01, -7.4364e-03, -4.5854e-01, -3.5639e-02,\n",
      "         2.3643e-01, -2.8836e-01,  2.1521e-01, -1.3486e-01, -1.6413e+00,\n",
      "        -2.6091e-01,  3.2434e-02,  5.6621e-02, -4.3296e-02, -2.1672e-02,\n",
      "         2.2476e-01, -7.5129e-02, -6.7018e-02, -1.4247e-01,  3.8825e-02,\n",
      "        -1.8951e-01,  2.9977e-01,  3.9305e-01,  1.7887e-01, -1.7343e-01,\n",
      "        -2.1178e-01,  2.3617e-01, -6.3681e-02, -4.2318e-01, -1.1661e-01,\n",
      "         9.3754e-02,  1.7296e-01, -3.3073e-01,  4.9112e-01, -6.8995e-01,\n",
      "        -9.2462e-02,  2.4742e-01, -1.7991e-01,  9.7908e-02,  8.3118e-02,\n",
      "         1.5299e-01, -2.7276e-01, -3.8934e-02,  5.4453e-01,  5.3737e-01,\n",
      "         2.9105e-01, -7.3514e-03,  4.7880e-02, -4.0760e-01, -2.6759e-02,\n",
      "         1.7919e-01,  1.0977e-02, -1.0963e-01, -2.6395e-01,  7.3990e-02,\n",
      "         2.6236e-01, -1.5080e-01,  3.4623e-01,  2.5758e-01,  1.1971e-01,\n",
      "        -3.7135e-02, -7.1593e-02,  4.3898e-01, -4.0764e-02,  1.6425e-02,\n",
      "        -4.4640e-01,  1.7197e-01,  4.6246e-02,  5.8639e-02,  4.1499e-02,\n",
      "         5.3948e-01,  5.2495e-01,  1.1361e-01, -4.8315e-02, -3.6385e-01,\n",
      "         1.8704e-01,  9.2761e-02, -1.1129e-01, -4.2085e-01,  1.3992e-01,\n",
      "        -3.9338e-01, -6.7945e-02,  1.2188e-01,  1.6707e-01,  7.5169e-02,\n",
      "        -1.5529e-02, -1.9499e-01,  1.9638e-01,  5.3194e-02,  2.5170e-01,\n",
      "        -3.4845e-01, -1.0638e-01, -3.4692e-01, -1.9024e-01, -2.0040e-01,\n",
      "         1.2154e-01, -2.9208e-01,  2.3353e-02, -1.1618e-01, -3.5768e-01,\n",
      "         6.2304e-02,  3.5884e-01,  2.9060e-02,  7.3005e-03,  4.9482e-03,\n",
      "        -1.5048e-01, -1.2313e-01,  1.9337e-01,  1.2173e-01,  4.4503e-01,\n",
      "         2.5147e-01,  1.0781e-01, -1.7716e-01,  3.8691e-02,  8.1530e-02,\n",
      "         1.4667e-01,  6.3666e-02,  6.1332e-02, -7.5569e-02, -3.7724e-01,\n",
      "         1.5850e-02, -3.0342e-01,  2.8374e-01, -4.2013e-02, -4.0715e-02,\n",
      "        -1.5269e-01,  7.4980e-02,  1.5577e-01,  1.0433e-01,  3.1393e-01,\n",
      "         1.9309e-01,  1.9429e-01,  1.5185e-01, -1.0192e-01, -1.8785e-02,\n",
      "         2.0791e-01,  1.3366e-01,  1.9038e-01, -2.5558e-01,  3.0400e-01,\n",
      "        -1.8960e-02,  2.0147e-01, -4.2110e-01, -7.5156e-03, -2.7977e-01,\n",
      "        -1.9314e-01,  4.6204e-02,  1.9971e-01, -3.0207e-01,  2.5735e-01,\n",
      "         6.8107e-01, -1.9409e-01,  2.3984e-01,  2.2493e-01,  6.5224e-01,\n",
      "        -1.3561e-01, -1.7383e-01, -4.8209e-02, -1.1860e-01,  2.1588e-03,\n",
      "        -1.9525e-02,  1.1948e-01,  1.9346e-01, -4.0820e-01, -8.2966e-02,\n",
      "         1.6626e-01, -1.0601e-01,  3.5861e-01,  1.6922e-01,  7.2590e-02,\n",
      "        -2.4803e-01, -1.0024e-01, -5.2491e-01, -1.7745e-01, -3.6647e-01,\n",
      "         2.6180e-01, -1.2077e-02,  8.3190e-02, -2.1528e-01,  4.1045e-01,\n",
      "         2.9136e-01,  3.0869e-01,  7.8864e-02,  3.2207e-01, -4.1023e-02,\n",
      "        -1.0970e-01, -9.2041e-02, -1.2339e-01, -1.6416e-01,  3.5382e-01,\n",
      "        -8.2774e-02,  3.3171e-01, -2.4738e-01, -4.8928e-02,  1.5746e-01,\n",
      "         1.8988e-01, -2.6642e-02,  6.3315e-02, -1.0673e-02,  3.4089e-01,\n",
      "         1.4106e+00,  1.3417e-01,  2.8191e-01, -2.5940e-01,  5.5267e-02,\n",
      "        -5.2425e-02, -2.5789e-01,  1.9127e-02, -2.2084e-02,  3.2113e-01,\n",
      "         6.8818e-02,  5.1207e-01,  1.6478e-01, -2.0194e-01,  2.9232e-01,\n",
      "         9.8575e-02,  1.3145e-02, -1.0652e-01,  1.3510e-01, -4.5332e-02,\n",
      "         2.0697e-01, -4.8425e-01, -4.4706e-01,  3.3305e-03,  2.9264e-03,\n",
      "        -1.0975e-01, -2.3325e-01,  2.2442e-01, -1.0503e-01,  1.2339e-01,\n",
      "         1.0978e-01,  4.8994e-02, -2.5157e-01,  4.0319e-01,  3.5318e-01,\n",
      "         1.8651e-01, -2.3622e-02, -1.2734e-01,  1.1475e-01,  2.7359e-01,\n",
      "        -2.1866e-01,  1.5794e-02,  8.1754e-01, -2.3792e-02, -8.5469e-01,\n",
      "        -1.6203e-01,  1.8076e-01,  2.8014e-02, -1.4340e-01,  1.3139e-03,\n",
      "        -9.1735e-02, -8.9704e-02,  1.1105e-01, -1.6703e-01,  6.8377e-02,\n",
      "        -8.7388e-02, -3.9789e-02,  1.4184e-02,  2.1187e-01,  2.8579e-01,\n",
      "        -2.8797e-01, -5.8996e-02, -3.2436e-02, -4.7009e-03, -1.7052e-01,\n",
      "        -3.4741e-02, -1.1489e-01,  7.5093e-02,  9.9526e-02,  4.8183e-02,\n",
      "        -7.3775e-02, -4.1817e-01,  4.1268e-03,  4.4414e-01, -1.6062e-01,\n",
      "         1.4294e-01, -2.2628e+00, -2.7347e-02,  8.1311e-01,  7.7417e-01,\n",
      "        -2.5639e-01, -1.1576e-01, -1.1982e-01, -2.1363e-01,  2.8429e-02,\n",
      "         2.7261e-01,  3.1026e-02,  9.6782e-02,  6.7769e-03,  1.4082e-01,\n",
      "        -1.3064e-02, -2.9686e-01, -7.9913e-02,  1.9500e-01,  3.1549e-02,\n",
      "         2.8506e-01, -8.7461e-02,  9.0611e-03, -2.0989e-01,  5.3913e-02],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Initialize the word embeddings with Glove \n",
    "print('Loading Glove embeddings...')\n",
    "vocab = {}\n",
    "glove_embeddings = []\n",
    "with open('glove.6B/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split()\n",
    "        vocab[line[0]] = len(vocab)\n",
    "        glove_embeddings.append([float(x) for x in line[1:]])\n",
    "print('Done.')\n",
    "glove_embeddings = torch.tensor(glove_embeddings, dtype=torch.float, device=device)\n",
    "print(glove_embeddings.shape)\n",
    "print(glove_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For words not found in Glove, use random initializations.\n",
    "for token in ptb_train_tokens:\n",
    "    if token not in vocab.keys():\n",
    "        vocab[token] = len(vocab)\n",
    "        glove_embeddings = torch.cat((glove_embeddings, torch.randn(1, glove_embeddings.shape[1], device=device)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'feel', 'any', 'patient', 'with', 'high-risk', 'cancer', 'is', 'getting', 'short', '<unk>', 'if', 'he', 'is', 'not', 'offered', 'this', 'opportunity', '<eos>'], ['there', \"'s\", 'plenty', 'of', '<unk>', 'here', 'but', 'it', 'is', \"n't\", 'always', 'clear', 'whether', 'it', \"'s\", '<unk>', '<eos>'], ['partly', 'in', 'response', 'a', 'bipartisan', 'group', 'of', 'senators', 'from', 'the', 'finance', 'and', 'labor', 'committees', 'is', '<unk>', 'a', 'plan', 'to', 'attract', 'broader', 'support', '<eos>'], ['the', 'survey', 'reported', 'the', 'number', 'of', 'people', 'who', 'said', 'they', 'bowl', 'regularly', 'has', 'fallen', 'to', 'just', 'N', 'N', 'from', 'N', 'N', 'in', 'N', '<eos>'], ['mitsubishi', \"'s\", 'investment', 'in', 'free', 'state', 'is', 'very', 'small', 'less', 'than', '$', 'N', 'million', 'mr.', '<unk>', 'says', '<eos>']]\n",
      "tensor([    41,    998,    130,   3222,     17,  64253,   1647,     14,    881,\n",
      "           636, 400005,     83,     18,     14,     36,   1112,     37,   1539,\n",
      "        400004,     63], device='cuda:0')\n",
      "(tensor([  41,  998,  130, 3222,   17], device='cuda:0'), tensor([  998,   130,  3222,    17, 64253], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# test the vocab and dataloader\n",
    "ptb_train_dataset = PennTreebankDataset(ptb_train_lines, 5, 10)\n",
    "print(ptb_train_dataset.data_lines[:5])\n",
    "print(ptb_train_dataset.data[:20])\n",
    "print(ptb_train_dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Generating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test an LSTM Language Model\n",
    "class LSTM_LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_prob, glove_embeddings):\n",
    "        super(LSTM_LM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(glove_embeddings)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, dropout=self.dropout_prob, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embedding = self.embedding(inputs)\n",
    "        embedding = self.dropout(embedding)\n",
    "        lstm_out, hidden = self.lstm(embedding, hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        outputs = self.linear(lstm_out)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        \n",
    "    def reuse_hidden(self, hidden):\n",
    "        return (hidden[0].detach(), hidden[1].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test an RNN Language Model\n",
    "class RNN_LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_prob, glove_embeddings):\n",
    "        super(RNN_LM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.embedding.weight.data.copy_(glove_embeddings)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.rnn = nn.RNN(self.embedding_dim, self.hidden_dim, self.num_layers, dropout=self.dropout_prob, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        embedding = self.embedding(inputs)\n",
    "        embedding = self.dropout(embedding)\n",
    "        rnn_out, hidden = self.rnn(embedding, hidden)\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        outputs = self.linear(rnn_out)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "    \n",
    "    def reuse_hidden(self, hidden):\n",
    "        return hidden.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, train_dataLoader, criterion, optimizer, reuse=True):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    hidden = model.init_hidden(train_dataLoader.dataset.batch_size)\n",
    "    for inputs, targets in train_dataLoader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        if reuse:\n",
    "            hidden = model.reuse_hidden(hidden)\n",
    "        else:\n",
    "            hidden = model.init_hidden(train_dataLoader.dataset.batch_size)\n",
    "        model.zero_grad()\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs.view(-1, model.vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(train_dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test(model, test_dataLoader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    hidden = model.init_hidden(test_dataLoader.dataset.batch_size)\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_dataLoader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(outputs.view(-1, model.vocab_size), targets.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(test_dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "args = {\n",
    "    'vocab_size': len(vocab),\n",
    "    'embedding_dim': 300,\n",
    "    'hidden_dim': 300,\n",
    "    'num_layers': 2,\n",
    "    'dropout_prob': 0.5,\n",
    "    'shuffle': False,\n",
    "    'reuse_hidden': True,\n",
    "    'glove_embeddings': glove_embeddings,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 30,\n",
    "    'batch_size': 32,\n",
    "    'sequence_length': 10,\n",
    "    'save_path': 'assets/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 6.576207147503106, Valid Loss: 5.830596270169893, Valid Perplexity: 340.56168531641254\n",
      "Epoch: 1, Train Loss: 6.0530720659266555, Valid Loss: 5.609255682008969, Valid Perplexity: 272.9410074777149\n",
      "Epoch: 2, Train Loss: 5.929843282780035, Valid Loss: 5.5659644074715375, Valid Perplexity: 261.3771562801755\n",
      "Epoch: 3, Train Loss: 5.86322358856146, Valid Loss: 5.542021010784393, Valid Perplexity: 255.19322690675276\n",
      "Epoch: 4, Train Loss: 5.821831322313935, Valid Loss: 5.501334793299527, Valid Perplexity: 245.01876349339693\n",
      "Epoch: 5, Train Loss: 5.788870190505264, Valid Loss: 5.498876594844923, Valid Perplexity: 244.41719843370342\n",
      "Epoch: 6, Train Loss: 5.766210574533716, Valid Loss: 5.458055677384953, Valid Perplexity: 234.64076321846667\n",
      "Epoch: 7, Train Loss: 5.74587117956184, Valid Loss: 5.448584924715268, Valid Perplexity: 232.4290285117868\n",
      "Epoch: 8, Train Loss: 5.732465921582644, Valid Loss: 5.454026793274111, Valid Perplexity: 233.69732455445177\n",
      "Epoch: 9, Train Loss: 5.72094327106508, Valid Loss: 5.437774439350812, Valid Perplexity: 229.92989068599312\n",
      "Epoch: 10, Train Loss: 5.713603718822679, Valid Loss: 5.428906853800487, Valid Perplexity: 227.89998120938327\n",
      "Epoch: 11, Train Loss: 5.704583060112193, Valid Loss: 5.426127812058005, Valid Perplexity: 227.26751687792776\n",
      "Epoch: 12, Train Loss: 5.701320933904898, Valid Loss: 5.424011363809232, Valid Perplexity: 226.7870255866619\n",
      "Epoch: 13, Train Loss: 5.694844720104454, Valid Loss: 5.423875304340954, Valid Perplexity: 226.75617116361408\n",
      "Epoch: 14, Train Loss: 5.694815011391096, Valid Loss: 5.424154120738021, Valid Perplexity: 226.8194033169416\n",
      "Epoch: 15, Train Loss: 5.689565017729617, Valid Loss: 5.430878401527289, Valid Perplexity: 228.34974011416358\n",
      "Epoch: 16, Train Loss: 5.68923218608276, Valid Loss: 5.427944954405439, Valid Perplexity: 227.68086975380933\n",
      "Epoch: 17, Train Loss: 5.686283520027136, Valid Loss: 5.418296282052269, Valid Perplexity: 225.49461582547937\n",
      "Epoch: 18, Train Loss: 5.687256916599637, Valid Loss: 5.419696989030461, Valid Perplexity: 225.81068901869705\n",
      "Epoch: 19, Train Loss: 5.687316480100112, Valid Loss: 5.424982786903265, Valid Perplexity: 227.0074387806444\n",
      "Epoch: 20, Train Loss: 5.688124238223621, Valid Loss: 5.415060976718334, Valid Perplexity: 224.76625076912995\n",
      "Epoch: 21, Train Loss: 5.6883779553569465, Valid Loss: 5.431001792200431, Valid Perplexity: 228.37791808072149\n",
      "Epoch: 22, Train Loss: 5.6897312901226345, Valid Loss: 5.432568868967538, Valid Perplexity: 228.7360843740079\n",
      "Epoch: 23, Train Loss: 5.694459024582406, Valid Loss: 5.435819862099041, Valid Perplexity: 229.48091387502507\n",
      "Epoch: 24, Train Loss: 5.690934502782521, Valid Loss: 5.430530746657073, Valid Perplexity: 228.2703670129225\n",
      "Epoch: 25, Train Loss: 5.691399995031975, Valid Loss: 5.429959482697368, Valid Perplexity: 228.14000161925009\n",
      "Epoch: 26, Train Loss: 5.696407321563081, Valid Loss: 5.432376751421433, Valid Perplexity: 228.69214437972948\n",
      "Epoch: 27, Train Loss: 5.695907142829482, Valid Loss: 5.432451980092243, Valid Perplexity: 228.70934923291688\n",
      "Epoch: 28, Train Loss: 5.698272862414849, Valid Loss: 5.439563270035486, Valid Perplexity: 230.34156442730952\n",
      "Epoch: 29, Train Loss: 5.698762488095092, Valid Loss: 5.436949266126453, Valid Perplexity: 229.74023695607056\n"
     ]
    }
   ],
   "source": [
    "## Train \n",
    "train_dataLoader = DataLoader(dataset=PennTreebankDataset(ptb_train_lines, args['sequence_length'], args['batch_size']), batch_size=args['batch_size'], shuffle=args['shuffle'])\n",
    "valid_dataLoader = DataLoader(dataset=PennTreebankDataset(ptb_valid_lines, args['sequence_length'], args['batch_size']), batch_size=args['batch_size'], shuffle=False)\n",
    "model = RNN_LM(args['vocab_size'], args['embedding_dim'], args['hidden_dim'], args['num_layers'], args['dropout_prob'], args['glove_embeddings'])\n",
    "model.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "for epoch in range(args['epochs']):\n",
    "    train_dataLoader.dataset.init_epoch()\n",
    "    train_loss = train(model, train_dataLoader, criterion, optimizer, reuse=args['reuse_hidden'])\n",
    "    valid_loss = test(model, valid_dataLoader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_perplexity = math.exp(valid_loss)\n",
    "    print('Epoch: {}, Train Loss: {}, Valid Loss: {}, Valid Perplexity: {}'.format(epoch, train_loss, valid_loss, valid_perplexity))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot curves and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot the loss curve and save the model\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(valid_losses, label='valid loss')\n",
    "plt.legend()\n",
    "## save with utc time\n",
    "torch.save(model.state_dict(), args['save_path'] + 'models/RNN_LM_' + datetime.utcnow().strftime(\"%Y-%m-%d-%H%MZ\") + '.pt')\n",
    "plt.savefig(args['save_path'] + 'curves/RNN_loss_curve_' + datetime.utcnow().strftime(\"%Y-%m-%d-%H%MZ\") + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.3347, Valid Loss: 5.4381, Test Perplexity: 207.4141, Valid Perplexity: 230.0043\n"
     ]
    }
   ],
   "source": [
    "## Test\n",
    "test_dataLoader = DataLoader(dataset=PennTreebankDataset(ptb_test_lines, args['sequence_length'], args['batch_size']), batch_size=args['batch_size'], shuffle=False)\n",
    "valid_dataLoader = DataLoader(dataset=PennTreebankDataset(ptb_valid_lines, args['sequence_length'], args['batch_size']), batch_size=args['batch_size'], shuffle=False)\n",
    "model = LSTM_LM(args['vocab_size'], args['embedding_dim'], args['hidden_dim'], args['num_layers'], args['dropout_prob'], args['glove_embeddings'])\n",
    "model.load_state_dict(torch.load(args['save_path'] + 'models/case3_LM_2023-10-24-0811Z.pt'))\n",
    "model.to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "test_loss = test(model, test_dataLoader, criterion)\n",
    "valid_loss = test(model, valid_dataLoader, criterion)\n",
    "test_perplexity = math.exp(test_loss)\n",
    "valid_perplexity = math.exp(valid_loss)\n",
    "print('Test Loss: {:.4f}, Valid Loss: {:.4f}, Test Perplexity: {:.4f}, Valid Perplexity: {:.4f}'.format(test_loss, valid_loss, test_perplexity, valid_perplexity))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, seed, length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor([[vocab[token] for token in seed]]).to(device)\n",
    "        outputs = [vocab[token] for token in seed]\n",
    "        for i in range(length):\n",
    "            hidden = model.init_hidden(1)\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            output = output.view(-1, model.vocab_size)\n",
    "            predict_prob = torch.exp(output[-1])\n",
    "            sample = torch.multinomial(predict_prob, 1)\n",
    "            outputs.append(sample.item())\n",
    "            inputs = torch.tensor([outputs]).to(device)\n",
    "            \n",
    "    return ' '.join([list(vocab.keys())[list(vocab.values()).index(i)] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the company will charge $ N billion in acquisitions it results will reach its income per the share to $\n"
     ]
    }
   ],
   "source": [
    "best_model = LSTM_LM(args['vocab_size'], args['embedding_dim'], args['hidden_dim'], args['num_layers'], args['dropout_prob'], args['glove_embeddings'])\n",
    "best_model.load_state_dict(torch.load(args['save_path'] + 'models/case3_LM_2023-10-24-0811Z.pt'))\n",
    "best_model.to(device)\n",
    "seed = ['the','company', 'will', 'charge', '$']\n",
    "print(generate(best_model, seed, 15))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we were able to get advertisers to <unk> the <unk> of the <unk> <eos> the <unk> <unk>\n",
      "we were able to get advertisers to <unk> the <unk> of the <unk> <eos> the <unk> <unk>\n",
      "we were able to get advertisers to <unk> the <unk> of the <unk> <eos> the <unk>\n",
      "we were able to get advertisers to <unk> the <unk> of the <unk> <eos> the <unk>\n",
      "we were <unk> <eos> the <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "and som <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "are you <unk> <eos> the <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "beauty is <unk> <eos> the <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m sequence \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m()\n\u001b[1;32m      3\u001b[0m seed \u001b[39m=\u001b[39m sequence\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit()\n\u001b[0;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(generate(best_model,seed, \u001b[39m10\u001b[39;49m))\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, seed, length)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length):\n\u001b[1;32m      7\u001b[0m     hidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_hidden(\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     output, hidden \u001b[39m=\u001b[39m model(inputs, hidden)\n\u001b[1;32m      9\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mvocab_size)\n\u001b[1;32m     10\u001b[0m     idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(output\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mLSTM_LM.forward\u001b[0;34m(self, inputs, hidden)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs, hidden):\n\u001b[0;32m---> 21\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(inputs)\n\u001b[1;32m     22\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embedding)\n\u001b[1;32m     23\u001b[0m     lstm_out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(embedding, hidden)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sequence = input()\n",
    "    seed = sequence.strip().split()\n",
    "    print(generate(best_model,seed, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bf9b6708bea9f75880765e6879768f88c54db0b01e9f79b6f29681d96d3dcd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
